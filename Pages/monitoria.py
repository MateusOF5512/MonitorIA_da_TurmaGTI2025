import streamlit as st
from groq import Groq
from typing import Generator
from supabase import create_client
import time
import pandas as pd
import json
from Functions.database import *


# VARIAVEIS PRICIPAIS:

if "mensagem" not in st.session_state:
    st.session_state.mensagem = []

if "selected_model" not in st.session_state:
    st.session_state.selected_model = []

# Para armazenar m√©tricas
if "metrics" not in st.session_state:
    st.session_state.metrics = {"input_tokens": 0, "output_tokens": 0, "elapsed": 0}

conta = st.session_state.usuario_logado


# Fun√ß√£o para buscar dados
db = get_data()
# Trnasformar os dados para json para facilitar leitura do modelo de LLM
df = pd.DataFrame(db)
colunas_para_usar = ["id", "conteudo", "usuario", "disciplina"]
df = df[df["disciplina"] == "Infraestrutura de Redes"]

df = df[[c for c in colunas_para_usar if c in df.columns]]
conteudos_json = df.to_dict(orient="records")
dados = json.dumps(conteudos_json, ensure_ascii=False, indent=2)

client = Groq(api_key=st.secrets["grok"]["GROK_API"],
              default_headers={"Groq-Model-Version": "latest"})

def generate_chat_responses(chat_completion) -> Generator[str, None, None]:
    for chunk in chat_completion:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

with st.sidebar:

    with st.container(border=True):
        models = {
            "groq/compound-mini": {"name": "Gorq | Compound Mini | 09/2023", "tokens": 8192},
            "groq/compound": {"name": "Gorq | Compound | 09/2023", "tokens": 8192},
            "openai/gpt-oss-20b": {"name": "OpenAI | GPT OSS 20B | 08/2025", "tokens": 65536},
            "openai/gpt-oss-120b": {"name": "OpenAI | GPT OSS 120B | 08/2025", "tokens": 65536},
            "gemma2-9b-it": {"name": "Google | Gemma 2 9B | 09/2023", "tokens": 8192},
            "llama-3.3-70b-versatile": {"name": "Meta | Llama 3.3 70B | 12/2024", "tokens": 32768},
            "meta-llama/llama-4-maverick-17b-128e-instruct": {"name": "Meta | Llama 4 Maverick 17B | 04/2025", "tokens": 8192,},
            "qwen/qwen3-32b": {"name":"Alibaba | Qwen3 32B | 05/2025", "tokens": 16384},
            "deepseek-r1-distill-llama-70b": {"name": "DeepSeek | R1 Llama 70B | 01/2025", "tokens": 4096}
        }

        model_options = st.selectbox(
                "Selecione um Modelo de Linguagem Natural:",
                options=list(models.keys()),
                format_func=lambda x: models[x]["name"],
                index=0
            )

        if st.session_state.selected_model != model_options:
            st.session_state.mensagem = []
            st.session_state.selected_model = model_options
            st.rerun()

        max_tokens_range = models[model_options]["tokens"]

        max_tokens = st.slider("M√°ximo de Tokens para o Modelo:",
                               min_value=512, max_value=max_tokens_range,
                               value=int(max_tokens_range/2), step=512)

        temperature = st.slider("Selecione a Criatividade do Modelo:",
                               min_value=0.1, max_value=2.0,
                               value=0.4, step=0.2)

    col1, col2, col3 = st.columns([1, 3, 1])
    with col1:
        st.markdown("")
    with col2:
        if st.button("Reiniciar conversa"):
            st.session_state.mensagem = []
            st.session_state.metrics = {"input_tokens": 0, "output_tokens": 0, "elapsed": 0}
            st.rerun()
    with col3:
        st.markdown("")


st.subheader("üë®‚Äçüè´ MonitorIA da Turma", divider="rainbow", anchor=False)

for mensagem in st.session_state.mensagem:
    avatar = "üë®‚Äçüè´" if mensagem["role"] == "assistant" else "üë®‚Äçüéì"
    with st.chat_message("role", avatar=avatar):
        st.markdown(mensagem["content"])

if prompt := st.chat_input("Pergunte algo sobre as aulas do curso de Gest√£o de TI..."):
    st.session_state.mensagem.append({"role": "user", "content": prompt})

    with st.chat_message("user", avatar="üë®‚Äçüéì"):
        st.markdown(prompt)

    conteudos_texto = dados
    system = f"""
    Voc√™ √© o Monitor da Turma, um assistente acad√™mico experiente que fornece informa√ß√µes a turma de Gest√£o de TI sobre os cadernos e anota√ß√µes das aulas, paciente e altamente did√°tico do curso de Gest√£o da Tecnologia da Informa√ß√£o do IFSC Florian√≥polis. 
    Seu papel √© **auxiliar os estudantes exclusivamente com base nos dados fornecidos abaixo** (cadernos/arquivos do banco). Siga estas instru√ß√µes estritas:
    
    0. Entenda seu papel como Monitor da Turma, responda de froma clara e educada, se apresente se necess√°rio.
    1. Responda sempre em **portugu√™s** e em **apenas um ou dois par√°grafos** de forma clara, objetiva e did√°tica.
    2. Use **negrito** (Markdown `**texto**`) para destacar palavras-chave e conceitos essenciais.
    3. **Baseie sua resposta unicamente** nas informa√ß√µes contidas em `conteudos_texto`. **N√£o** gere, suponha ou invente fatos que n√£o estejam no banco de dados.
    4. Analise cuidadosamente os registros dispon√≠veis; se a resposta vier da s√≠ntese de v√°rios registros, sintetize-os / resuma e **cite os IDs usados** ao final do par√°grafo no formato: `[IDs: ID1, ID2; usu√°rio: NOME (se dispon√≠vel); disciplina: NOME (se dispon√≠vel); data: AAAA-MM-DD (se dispon√≠vel)]`, use ITALICO para destacar as referncias, sempre citas as referencias no final da resposta ou depois de uma explica√ß√£o.
    4.1 Se sua resposta final nao tiver disponivel no conteudos_texto n√£o gere referencias, fale "Sem referencias no material dispon√≠vel" em negrito, analise com cuidado se sua resposta tem referencia ou nao e informe com exatidao ao usuario
    5. Sempre que poss√≠vel, extraia e apresente ‚Äî dentro do mesmo par√°grafo ‚Äî as **refer√™ncias** indicando de qual conte√∫do a resposta foi gerada: **ID do registro**, **autor/usu√°rio** que gerou o caderno (se existir), **disciplina** e **data da aula**. Se algum metadado estiver ausente, indique explicitamente `(metadado n√£o dispon√≠vel)`.
    6. Se os dados forem insuficientes para fornecer uma resposta completa, responda em um par√°grafo dizendo claramente que **n√£o h√° informa√ß√£o suficiente** e sugira quais informa√ß√µes ou quais IDs/arquivos adicionais s√£o necess√°rios.
    7. Se houver entradas conflitantes nos dados, aponte a contradi√ß√£o e cite os IDs conflitantes.
    8. Evite jarg√µes desnecess√°rios; priorize uma explica√ß√£o passo-a-passo simples e acion√°vel quando √∫til.
    9. Mantenha um tom acolhedor, respeitoso e voltado ao aprendizado do aluno.

    Dados a serem usados (n√£o altere): {conteudos_texto}
    """

    try:
        start_time = time.time()

        chat_completion = client.chat.completions.create(
            model=model_options,
            messages=[{"role": "system", "content": system}] + st.session_state.mensagem,
            max_completion_tokens=max_tokens,
            temperature=temperature,
            stream=True,
        )

        with st.chat_message("assistant", avatar='üë®‚Äçüè´'):
            chat_responses_generator = generate_chat_responses(chat_completion)
            full_response = st.write_stream(chat_responses_generator)

        elapsed = time.time() - start_time


        # --- Estimativa de tokens (simples e incluindo o texto do sistema) ---
        # 1 token ‚âà 4 caracteres em m√©dia (estimativa usada pela OpenAI)
        def estimar_tokens(texto):
            return len(texto) / 4  # aproxima√ß√£o leve, sem depender de tiktoken


        # Soma de todos os textos do usu√°rio e do sistema
        entrada_total = system + " ".join(m["content"] for m in st.session_state.mensagem)

        input_tokens = int(estimar_tokens(entrada_total))
        output_tokens = int(estimar_tokens(full_response))

        st.session_state.metrics = {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "elapsed": elapsed
        }

    except Exception as e:
        st.error(e)

    try:
        if isinstance(full_response, str):
            st.session_state.mensagem.append({"role": "assistant", "content": full_response})
        else:
            combined_response = '\n'.join(str(item) for item in full_response)
            st.session_state.mensagem.append({"role": "assistant", "content": combined_response})

    except:
        st.error("Max√≠mo de Tokens desse Modelo alcan√ßado! Teste daqui a pouco.")

    col1, col2, col3 =  st.columns(3)
    with col1:
        st.write(f"üîπ Tokens de Entrada: **{st.session_state.metrics['input_tokens']}**")
    with col2:
        st.write(f"üîπ Tokens de Sa√≠da: **{st.session_state.metrics['output_tokens']}**")
    with col3:
        st.write(f"‚è±Ô∏è Tempo de Resposta: **{st.session_state.metrics['elapsed']:.2f}s**")

# --- Exibir m√©tricas no sidebar ---

